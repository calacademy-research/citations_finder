[general]
logging_level = INFO
report_start_year = 2020
report_end_year = 2020
# Dumps stats on the papers downloaded from the year range above
report_on_start = False
exit_after_report = False
# For testing, generally.
#------------------------------------------------------
download_single_doi_mode = False
download_single_doi = 10.3390/plants10102053
#------------------------------------------------------


# iterates only items that failed to download with populated
# unpaywall_downloader url fields. Recommended with firefox_downloader enabled
# When enabled, it skips all other steps and ONLY does the retry, and exits
# once all failed downloads have been retried.
retry_failed_unpaywall_links = False


#------------------------------------------------------
# Used to generate a new output file - if there are dois for the
# journal, then it makes it into this file.
# The workflow idea is that we add all posisble jounrals (which picks up web
# issns, paper, electronic, etc). Most of these have no actual paper dois associated
# with them, so they clutter up the journals.tsv file. Run a full DOI query against
# crossref for a single year, then enable this funciton to limit journals.tsv to only
# interesting ISSNs, copy the generated .tsv over journals.tsv
#and that will speed up subsequent year queries against crossref.
write_used_journals = True
used_journals_only_file = used_journals_only_file.tsv


[journal population]
# Scans GBIF for journal ISSNs to populate journals.tsv.
# uses GBIF's pattern matching for instutitoins (CAS) which tends to cast a too-wide
# net, if using this method, manually editing journals.tsv to remove non-germane publications
# is advised.
populate_journals = False
gbif_api_collection_links = ["https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=f934f8e2-32ca-46a7-b2f8-b032a4740454&limit=1000",
                             "https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=5d6c10bd-ea31-4363-8b79-58c96d859f5b&limit=1000",
                             "https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=5e848873-567f-42dd-8a29-88056b9b33a4&limit=1000"
                            ]

# Source for the DOIs
[crossref]
# Skip this step entirely - do not update dois from crossref.
skip_crossref = True
# This is a reporting step that is for information only, and can take a while
# to run, but will let you know what will be downloaded from crossref.
# enable to get a full report on what needs to be downloded before the download starts
skip_crossref_precheck = True


# Force update is useful if re-scanning an incomplete year - e.g.:
# Last time we ran was march, 2022 and it is now 2023. The standard
# "scan_for_dois_after_year" and "scan_for_dois_before_year" would skip
# 2022 in this case, because entries in that year already exist.
# Doi entries are self checking and will never be duplicated, so this is safe.

# Note: force update will override "skip_crossref"!
force_update = False
force_update_year = 2020



scan_for_dois_after_year = 2020
scan_for_dois_before_year = 2021




# The 'download' step comes after the DOIs have been downloaded.
# For each DOI, the system checks to see if it's marked in the database as downloaded
# If not, it checks to see if the PDF exists on disk. If not, it
# attempts to download using the enabled downloader(s).
# if both the download_single_journal and download_all_journals are false, this step is skipped.
[download]
download_start_year = 2021
download_end_year = 2021

# Slow! Iterates over every DOI and checks the file databse in
# ./pdf. If there is a pdf available, link it to the doi record. If not,
# purge what's there and mark it as missing.
updatate_pdf_file_link = False
# it is possible to enable single journal AND all journals mode, in which case
# single journal will run first.
download_single_journal = False
download_single_journal_issn = 0373-6687
enable_paper_download = False

# note, many options under "download" apply here.
[downloaders]
pdf_directory = ./pdf
header_email = curator@museum.org
modules = ["unpaywall_downloader"]
#requires absolute path. Will get a .{pid} suffix. Directory and contents will be deleted.
tmp_firefox_pdf_directory = /Users/joe/citations_finder/temp_firefox_pdf
# 0 is an acceptable value
firefox_page_load_timeout = 10

# TODO: Archive.org downloader

# desirable if there are multiple downloaders running concurrently
# minimizes the possibility of conflict
randomize_download_order = False

# Suppress journal report header (nice to see, but it's slow)
suppress_journal_report_header = True

[unpaywall_downloader]

# Do not retry
# If there's any entry at all in unpaywall_downloader for this DOI (i.e: we attenpted in the
# past at ANY time) don't try again.
do_not_retry=True


# If true, checks the database field "most_recent_attempt" in the
# relevant download table (usually "unpaywall_downloader"). If
# a "recent" (i.e.: after "retry after datetime" field below) attempt
# was made, don't retry. The intent is that nothing is likely to have
# changed if we tried recently, so don't keep hammering on closed doors.
use_datetime_restriction = False
# %m/%d/%Y %H:%M:%S
retry_after_datetime = 8/1/2023 12:00:00

#------------------------------------------------------

# When we scrape unpaywall, we sometimes don't get a hit on a DOI.
# We still create a database entry, but leave the "open_url" column blank.
# If we retry an unpawywall download, we can re-scrape unpaywall for this
# doi in case there's now a link when previously there was not.
# Most of the time, it doesn't make sense to requery unpaywall again
# - no reaon to think we have a link now when we didn't previously.
# However, if it's been a while, it might be worth requerying unpaywall
# in case a link to this paper has been added.
# Note: ignores retry_after_datetime restrictions
retry_only_failures_with_link = False

# normally we only attempt to fetch download link from
# unpaywall when the link isn't already in the database
force_open_url_update = False

# If an entry is marked as having been attemped and there's no unpaywall link,
# do not attempt to re-fetch it under any circumstances.
do_not_refetch_links = False
#------------------------------------------------------

# to get a sense of what percentages of papers have unpaywall links, usually
# Does not attempt a paper download. This is an override - rarely do
# we want this to be true.
force_update_link_only = False

# populate_not_available_only - no effect if force_update_link_only is False
# Skips all steps if the unpaywall_downloader -> 'not_available' column is not null
# note, this also has the side effect of scanning files for the "already downloaded" cases
# that haven't been marked.
populate_not_available_only = False
#------------------------------------------------------


# Often, open source journals will provide a direct link, but this usually goes
# to an HTML version. Picking the actual URL out of that isn't implemented because it's
# easier to just go straight to unpaywall.
attempt_direct_link = False
# Use selenium to download firefiox - attempted if we get html response, which
# likely indicates an interception by cloudflare. Try it with a full user browser
# controlled through selenium
# requires gekodriver. Install on mac with "brew install geckodriver"
# Enable terminal and/or pycharm to control keyboard via security
#------------------------------------------------------

firefox_downloader = True
retry_firefox_failure = True
re_used_direct_url_sleep_time = 30


# (re)Scans the existing PDFs for regex matches.
[scan]
enabled = False
scan_start_year = 2016
scan_end_year = 2016

# "reset" causes a the whole scan database to be rebuilt.
# required most of the time- it won't pick up new PDFs without it
# (this comment taken from the code- needs verification)
reset_scan_database = False

# re-run scoring algorithm (typically after code changes)
rescore = False
# (re)scans papers for specimen IDs to map specimens back to papers published.


# location for the pdf->txt file conversion
scan_text_directory = ./txt

[scan_for_specimen_ids]
enabled = False
reset_scan_database = False

# interactive validate step
[validate]
enabled = False
regular_prompts = False
# doesn't work until the regular validation sequence is completed.
digital_prompts = False
validate_start_year = 2015
validate_end_year = 2021

# copy identified PDFs to unique directory
# export TSV summary of scan results
[copyout]
enabled = False
target_dir = ./publish
copyout_start_year = 2016
copyout_end_year = 2016
copyout_pdfs = True
export_tsv = True

[scan_search_keys]
institution_root_name = 'cas'
collections_with_id_strings = ['ent', 'c', 'iz']

collection_manager_names = [
    ("J. vindum", 1000),
    ("christopher grinter", 1000),
    ("christina piotrowski", 1000),
    ("johanna loacker", 1000),
    ("maricela abarca", 200),
    ("Christine Garcia",200),
    ("David Catania", 1000),
    ("lauren scheinberg", 1000),
    ("shevock", 200),
    ("j. shevock", 300),
    ("seth cotterell", 300),
    ("jon fong", 200),
    ("wojciech pulawski", 100),
    ("D.H. Kavanaugh", 200),
    ("C.E. Griswold", 200),
    ]

scored_strings = [
    ('california academy of science[s]?', 200),
    ('southern california academy of science[s]?', -200),
    ("CASC", 60),
    ("CASIZ", 200),
    ("izcas", -200),
    ("CAS-SUA", 200),
    ("CAS-SUR", 200),
    ("CAS-SU", 200),
    ("CAS:SU", 200),
    ("CASG", 200),
    ("CAS:ICH", 200),
    ("CAS-ICH", 200),
    ("CASTYPE", 200),
    ("CASENT", 200),
    ("antweb", 400),
    ("antcat", 400),
    ("inaturalist", 400),
    ("catalog of fishes", 400),
    ("CAS", 20),
    ("center for comparative genomics", 100),
    ("CAS HERP", 100),
    ("CAS MAM", 100),
    ("CAS ORN", 100),
    ("chinese", -20),
    ("Chinese Academy of Sciences", -100),
    ("institute of botany cas", -200),
    ("biology centre cas", -200)
    ]
