[general]
logging_level = INFO
report_start_year = 2022
report_end_year = 2022
; Dumps stats on the papers downloaded from the year range above.
; This is the second step - after checking that the DOI database is populated from
; crossref entries. It can be time consuming (~ 5 minutes) and is purely informative.
report_on_start = False
exit_after_report = False
# For testing, generally.
download_single_doi_mode = False
download_single_doi = 10.1073/pnas.1719260115
; iterates only items that failed to download with populated
; unpaywall_downloader url fields. Recommended with firefox_downloader enabled
retry_failed_unpaywall_links = False

# Run through the PDF directory and populate the mysql database with available PDFs, if any.
# Use case is starting a new system with a tarball of papers that are stored
# in the directory structure created by an external instance.

# Papers in the PDF directory are stored in ISSN/YEAR/DOI.pdf.
do_pdf_ingest = False
pdf_ingest_directory = ./pdf

[journal population]
populate_journals = False 
gbif_api_collection_links = ["https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=f934f8e2-32ca-46a7-b2f8-b032a4740454&limit=1000",
                             "https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=5d6c10bd-ea31-4363-8b79-58c96d859f5b&limit=1000",
                             "https://api.gbif.org/v1/literature/search?contentType=literature&year=2021,2022&literatureType=journal&gbifDatasetKey=5e848873-567f-42dd-8a29-88056b9b33a4&limit=1000"
                            ]

; Source for the DOIs
[crossref]
;  Force an update - typically used for an existing database.
; If the DOIs have never been pulled, then the system will download them
; in the year range specified in scan_for_dois_after_year. However, unless the
; scan_for_dois_after_year in general is pushed farther back, it will never download them again.
; To pull the most recent papers, enable force update and specify the oldest year
; to update, and re-run.
force_update = False
force_update_year = 2022
scan_for_dois_after_year = 2014
scan_for_dois_before_year = 2020




; The 'download' step comes after the DOIs have been downloaded.
; For each DOI, the system checks to see if it's marked in the database as downloaded
; If not, it checks to see if the PDF exists on disk. If not, it
; attempts to download using the enabled downloader(s).
; if both the download_single_journal and download_all_journals are false, this step is skipped.
[download]
download_start_year = 2022
download_end_year = 2023
; it is possible to enable single journal AND all journals mode, in which case
; single journal will run first.

; if true, will try to use the configured downloaders to
; download any missing papers.
; Note - paper downloading is the most time consuming step; run time isn't bounded
; and can be weeks.
; If the papers already exist in the 'pdf' tree, this step will include those papers
; in the internal mysql database instead of attempting to download them
enable_paper_download = False

; test mode- download only from a single journal.
download_single_journal = False
download_single_journal_issn = 1932-6203
download_all_journals = True

; note, many options under "download" apply here.
[downloaders]
pdf_directory = ./pdf
header_email = curator@museum.org
modules = ["unpaywall_downloader"]
; Firefox ignores user setting for default save directory
firefox_save_directory = /Users/joe/Downloads

; TODO: Archive.org downloader
parallel_downloader = False

[unpaywall_downloader]
use_datetime_restriction = True
; %m/%d/%Y %H:%M:%S
retry_after_datetime = 10/9/2022 12:00:00
; ignores retry_after_datetime restrictions
retry_only_failures_with_link = True

; normally we only attempt to fetch download link from
; unpaywall when the link isn't already in the database
force_open_url_update = False

; If an entry is marked as having been attempted and there's no unpaywall link,
; do not attempt to re-fetch it under any circumstances.
do_not_refetch_links = True

; to get a sense of what percentages of papers have unpaywall links, usually
; Does not attempt a paper download
force_update_link_only = False

; populate_not_available_only - no effect if force_update_link_only is False
; Skips all steps if the unpaywall_downloader is not_available is not null
; note, this also has the side effect of scanning files for the "already downloaded" cases
; that haven't been marked.
populate_not_available_only = True

# Often, open source journals will provide a direct link, but this usually goes
# to an HTML version. Picking the actual URL out of that isn't implemented because it's
# easier to just go straight to unpaywall.
attempt_direct_link = False
# Use selenium to download firefiox - attempted if we get html response, which
# likely indicates an interception by cloudflare. Try it with a full user browser
# controlled through selenium
# requires gekodriver. Install on mac with "brew install geckodriver"
# Enable terminal and/or pycharm to control keyboard via security

# WARNING - CURRENTLY NUKES ~/DOWNLOADS DIRECTORY!!
firefox_downloader = False
retry_firefox_failure = False

; 0 is an acceptable value
re_used_direct_url_sleep_time = 30


; (re)Scans the existing PDFs for regex matches.
[scan]
enabled = False
scan_start_year = 2022
scan_end_year = 2022

; "reset" causes a the whole scan database to be rebuilt.
; required most of the time; it won't pick up new PDFs without it
; if this isn't enabled, scan only runs against existing scan database records.
; (this comment taken from the code; needs verification)
reset_scan_database = False

; re-run scoring algorithm (typically after code changes)
rescore = False
; (re)scans papers for specimen IDs to map specimens back to papers published.


; location for the pdf->txt file conversion
scan_text_directory = ./txt

[scan_for_specimen_ids]
enabled = False
reset_scan_database = False

; interactive validate step
[validate]
enabled = True
regular_prompts = True
; doesn't work until the regular validation sequence is completed.
; y/n sequence to set "digital only" boolean flag for export.
digital_prompts = True
validate_start_year = 2022
validate_end_year = 2022

; copy identified PDFs to unique directory
; export TSV summary of scan results
[copyout]
enabled = True
target_dir = ./publish
copyout_start_year = 2022
copyout_end_year = 2022
copyout_pdfs = True
export_tsv = True


[scan_search_keys]
institution_root_name = 'cas'
collections_with_id_strings = ['ent', 'c', 'iz']

collection_manager_names = [
    ("J. vindum", 1000),
    ("christopher grinter", 1000),
    ("christina piotrowski", 1000),
    ("johanna loacker", 1000),
    ("maricela abarca", 200),
    ("Christine Garcia",200),
    ("David Catania", 1000),
    ("lauren scheinberg", 1000),
    ("shevock", 200),
    ("j. shevock", 300),
    ("seth cotterell", 300),
    ("jon fong", 200),
    ("wojciech pulawski", 100),
    ("D.H. Kavanaugh", 200),
    ("C.E. Griswold", 200),
    ]

scored_strings = [
    ('california academy of science[s]?', 200),
    ('southern california academy of science[s]?', -200),
    ("CASC", 60),
    ("CASIZ", 200),
    ("izcas", -200),
    ("CAS-SUA", 200),
    ("CAS-SUR", 200),
    ("CAS-SU", 200),
    ("CAS:SU", 200),
    ("CASG", 200),
    ("CAS:ICH", 200),
    ("CAS-ICH", 200),
    ("CASTYPE", 200),
    ("CASENT", 200),
    ("antweb", 400),
    ("antcat", 400),
    ("catalog of fishes", 400),
    ("CAS", 20),
    ("center for comparative genomics", 100),
    ("CAS HERP", 100),
    ("CAS MAM", 100),
    ("CAS ORN", 100),
    ("chinese", -20),
    ("Chinese Academy of Sciences", -100),
    ("institute of botany cas", -200),
    ("biology centre cas", -200),
    ("crispr". -100),
    ("inaturalist", -100)
    ]
